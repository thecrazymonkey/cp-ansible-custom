all:
  vars:
#    confluent_repo_version: 5.4
#    confluent_package_version: 5.4.2
    ansible_connection: ssh
    ansible_user: centos
    ansible_become: true
    ansible_ssh_private_key_file: ~/.ssh/ivan.pem
    redhat_java_package_name: java-11-openjdk

    #### Setting Proxy Environment variables ####
    ## To set proxy env vars for the duration of playbook run, uncomment below block and set as necessary
    # proxy_env:
    #   http_proxy: http://proxy.example.com:8080
    #   https_proxy: http://proxy.example.com:8080
    #   no_proxy: http://proxy.example.com:8080

    #### SASL Authentication Configuration ####
    ## By default there will be no SASL Authentication
    ## For SASL/PLAIN uncomment this line:
    # sasl_protocol: plain
    ## For SASL/SCRAM uncomment this line:
    # sasl_protocol: scram
    ## For SASL/GSSAPI uncomment this line and see Kerberos Configuration properties below
    # sasl_protocol: kerberos

    #### Zookeeper SASL Authentication ####
    ## Zookeeper can have Kerberos (GSSAPI) or Digest-MD5 SASL Authentication
    ## By default when sasl_protocol = kerberos, zookeeper will also use sasl kerberos. It can  be configured with:
    zookeeper_sasl_protocol: digest

    #### Kerberos Configuration ####
    ## Applicable when sasl_protocol is kerberos
    # kerberos_kafka_broker_primary: <Name of the primary set on the kafka brokers' principal eg. kafka>
    ## REQUIRED: Under each host set keytab file path and principal name, see below
    # kerberos_configure: <Boolean for ansible to install kerberos packages and configure this file: /etc/krb5.conf, defaults to true>
    # kerberos:
    #   realm: <KDC server realm eg. confluent.example.com>
    #   kdc_hostname: <hostname of machine with KDC running eg. ip-172-31-45-82.us-east-2.compute.internal>
    #   admin_hostname: <hostname of machine with KDC running eg. ip-172-31-45-82.us-east-2.compute.internal>

    #### TLS Configuration ####
    ## By default, data will NOT be encrypted. To turn on TLS encryption, uncomment this line
    ssl_enabled: true
    ## By default, the components will be configured with One-Way TLS, to turn on TLS mutual auth, uncomment this line:
    ssl_mutual_auth_enabled: true
    ## By default, the certs for this configuration will be self signed, to deploy custom certificates there are two options.
    ## Option 1: Custom Certs
    ## You will need to provide the path to the Certificate Authority Cert used to sign each hosts' certs
    ## As well as the signed certificate path and the key for that certificate for each host.
    ## These will need to be set for the correct host
    # ssl_custom_certs: true
    # ssl_custom_certs_remote_src: true # set to true if key crt and ca file already on hosts, file paths must still be set
    # ssl_ca_cert_filepath: "/tmp/certs/ca.crt" # Can be a bundle of ca certs to be included in truststore
    # ssl_signed_cert_filepath: "/tmp/certs/{{inventory_hostname}}-signed.crt" # Can be a full chain of certs
    # ssl_key_filepath: "/tmp/certs/{{inventory_hostname}}-key.pem"
    # ssl_key_password: <password for key for each host, will be inputting in the form -passin pass:{{ssl_key_password}} >
    ## Option 2: Custom Keystores and Truststores
    ## CP-Ansible can move keystores/truststores to their corresponding hosts and configure the components to use them. Set These vars
    ssl_provided_keystore_and_truststore: true
    ssl_keystore_filepath: "~/playground/ssl-certs/{{inventory_hostname}}.keystore.jks"
    ssl_keystore_key_password: test1234
    ssl_keystore_store_password: test1234
    ssl_truststore_filepath: "~/playground/ssl-certs/{{inventory_hostname}}.truststore.jks"
    ssl_truststore_password: test1234
    ssl_truststore_ca_cert_alias: caroot

    #### Certificate Regeneration ####
    ## When using self signed certificates, each playbook run will regenerate the CA, to turn this off, uncomment this line:
    # regenerate_ca: false
    ## By default, if keystore/truststore files exists for a component, the playbook will not recreate them
    ## To recreate the keystores and truststores uncomment this line:
    # regenerate_keystore_and_truststore: true

    #### Monitoring Configuration ####
    ## Jolokia is enabled by default. The Jolokia jar gets pulled from the internet and enabled on all the components
    ## If you plan to use the upgrade playbooks, it is recommended to leave jolokia enabled because kafka broker health checks depend on jolokias metrics
    ## To disable, uncomment this line:
    jolokia_enabled: true
    ## During setup, the hosts will download the jolokia agent jar from Maven. To update that jar download set this var
    # jolokia_jar_url: http://<inteneral-server>/jolokia-jvm-1.6.2-agent.jar
    ## JMX Exporter is disabled by default. When enabled, JMX Exporter jar will be pulled from the Internet and enabled on the broker *only*.
    ## To enable, uncomment this line:
    jmxexporter_enabled: true
    ## To update that jar download set this var
    # jmxexporter_jar_url: http://<internal-server>/jmx_prometheus_javaagent-0.12.0.jar

    #### Custom Yum Repo File (Rhel/Centos) ####
    ## If you are using your own yum repo server to host the packages, in the case of an air-gapped environment,
    ## use the below variables to distribute a custom .repo file to the hosts and skip our repo setup.
    ## Note, your repo server must host all confluent packages
    # custom_yum_repofile: true
    # custom_yum_repofile_filepath: /tmp/my-repo.repo

    #### Custom Apt Repo File (Ubuntu/Debian) ####
    ## If you are using your own apt repo server to host the packages, in the case of an air-gapped environment,
    ## use the below variables to distribute a custom .repo file to the hosts and skip our repo setup.
    ## Note, your repo server must host all confluent packages
    # custom_apt_repo: true
    # custom_apt_repo_filepath: "/tmp/my-source.list"

    #### Confluent Server vs Confluent Kafka ####
    ## Confluent Server will be installed by default, to install confluent-kafka instead, uncomment the below
    # confluent_server_enabled: false

    #### Schema Validation ####
    ## Schema Validation with the kafka configuration is disabled by default. To enable uncomment this line:
    ## Schema Validation only works with confluent_server_enabled: true
    # kafka_broker_schema_validation_enabled: true

    #### Fips Security ####
    ## To enable Fips for added security, uncomment the below line.
    ## Fips only works with ssl_enabled: true and confluent_server_enabled: true
    # fips_enabled: true

    #### Configuring Multiple Listeners ####
    ## CP-Ansible will configure two listeners on the broker: an internal listener for the broker to communicate and an external for the components and other clients.
    ## If you only need one listener uncomment this line:
    # kafka_broker_configure_additional_brokers: false
    ## By default both of these listeners will follow whatever you set for ssl_enabled and sasl_protocol.
    ## To configure different security settings on the internal and external listeners set the following variables:
    kafka_broker_custom_listeners:
#      internal:
#        name: INTERNAL
#        port: 9091
#        ssl_enabled: true
#        ssl_mutual_auth_enabled: true
#        sasl_protocol: none
#      external:
#        name: EXTERNAL
#        port: 9092
#        ssl_enabled: true
#        ssl_mutual_auth_enabled: true
#        sasl_protocol: none
    # You can even add additional listeners, make sure all variables are set
      client:
        name: CLIENT
        port: 9093
        ssl_enabled: true
        ssl_mutual_auth_enabled: true
        sasl_protocol: none
#      plain:
#        name: PLAIN
#        port: 9094
#        ssl_enabled: false
#        ssl_mutual_auth_enabled: false
#        sasl_protocol: none

    #### Creating Connectors ####
    ## To manage the connector configs from Ansible, set the following list of connector objects:
    ## one per connector, must have `name` and `config` properties
    ## make sure to provide the numeric values as strings
    # kafka_connect_connectors:
    #   - name: sample-connector
    #     config:
    #       connector.class: "FileStreamSinkConnector"
    #       tasks.max: "1"
    #       file: "path/to/file.txt"
    #       topics: "test_topic"

    #### Configuring Role Based Access Control ####
    ## To have CP-Ansible configure Components for RBAC and create necessary role bindings, set these Mandtory variables:
    ## Note: Confluent Components will be configured to connect to the "internal" listener automatically
    ## DO NOT UPDATE the internal listener
    ## Note: It is recommended to create an additional listener for external clients, but the interbroker one would also work
    ## Note: An authentication mode must be selected on all listeners, for example (ssl_enabled=false and ssl_mutual_auth_enabled=false) or sasl_protocol=none is not supported.
    # rbac_enabled: true
    ##
    ## LDAP Users
    ## Note: Below users must already exist in your LDAP environment.  See kafka_broker vars, for LDAP connection details.
    # mds_super_user: <Your mds super user which has the ability to bootstrap RBAC roles and permissions>
    # mds_super_user_password: <ldap password>
    # schema_registry_ldap_user: <Your Schema Registry username in LDAP>
    # schema_registry_ldap_password <Your schema  registry user's LDAP password>
    # kafka_connect_ldap_user: <Your Connect username in LDAP>
    # kafka_connect_ldap_password: <Your Connect user's password in LDAP>
    # ksql_ldap_user: <Your KSQL username in LDAP>
    # ksql_ldap_password: <Your KSQL user's password in LDAP>
    # kafka_rest_ldap_user: <Your REST Proxy's username in LDAP>
    # kafka_rest_ldap_password: <Your REST Proxy's password in LDAP>
    # control_center_ldap_user: <Your Control Center username in LDAP>
    # control_center_ldap_password: <Your Control Center password in LDAP>
    ## Below are optional variables
    # create_mds_certs: false # To provide your own MDS Certs set this variable and the next two
    # token_services_public_pem_file: /path/to/public.pem
    # token_services_private_pem_file: /path/to/tokenKeypair.pem
    # mds_acls_enabled: false #to turn off mds based acls, they are on by default if rbac is on
    # mds_ssl_enabled: true/false #defaults to whatever ssl_enabled var is set to
    # mds_ssl_mutual_auth_enabled: true/false #defaults to whatever ssl_mutual_auth_enabled var is set to
    ## Allow the playbooks to configure additional users as system admins on the platform, set the list below
    # rbac_component_additional_system_admins:
    #   - user1
    #   - user2
    ## By default the Confluent CLI will be installed on each host, to stop this download set:
    # confluent_cli_download_enabled: false
    ## CLI will be downloaded from Confluent's webservers, to customize the location of the binary set:
    # confluent_cli_custom_download_url: <URL to custom webserver hosting for confluent cli>

    ## To set custom properties for each service
    ## Find property options in the Confluent Documentation
    # zookeeper:
    #   properties:
    #     initLimit: 6
    #     syncLimit: 3
    kafka_broker:
      properties:
        num.io.threads: 8
        num.network.threads: 3
    # schema_registry:
    #   properties:
    #     key: val
    control_center:
      properties:
        confluent.controlcenter.deprecated.views.enable: true
        confluent.controlcenter.ui.autoupdate.enable: true
    # kafka_connect:
    #   properties:
    #     key: val
    # kafka_rest:
    #   properties:
    #     key: val
    # ksql:
    #   properties:
    #     key: val

    # from BDP calculator
    latency_rtt_ms: 60
    bandwidth_bps: 1000000000
    tuned_buffer_size: "{{ (bandwidth_bps*latency_rtt_ms/1000/8)|int}}"
zookeeper:
  hosts:
    zk[1:3].ivan.ps.confluent.io:
      ## By default the first host will get zookeeper id=1, second gets id=2. Set zookeeper_id to customize
      # zookeeper_id: 2

      ## For kerberos sasl protocol, EACH host will need these two variables:
      # zookeeper_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/zookeeper-ip-172-31-34-246.us-east-2.compute.internal.keytab>
      # zookeeper_kerberos_principal: <The principal configured in kdc server, eg. zookeeper/ip-172-31-34-246.us-east-2.compute.internal@REALM.EXAMPLE.COM>
    # ip-172-31-37-15.us-east-2.compute.internal:
      # zookeeper_id: 3
    # ip-172-31-34-231.us-east-2.compute.internal:
      # zookeeper_id: 1
kafka_broker:
  hosts:
      kafka1.ivan.ps.confluent.io:
        broker_id: 1
      kafka2.ivan.ps.confluent.io:
        broker_id: 2
      kafka3.ivan.ps.confluent.io:
        broker_id: 3
    # ip-172-31-34-246.us-east-2.compute.internal:
      ## By default the first host will get broker id=1, second gets id=2. Set broker_id to customize
      # broker_id: 3

      ## For kerberos sasl protocol, EACH host will need these two variables:
      # kafka_broker_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/ip-172-31-34-246.us-east-2.compute.internal>
      # kafka_broker_kerberos_principal: <The principal configured in kdc server, eg. kafka/ip-172-31-34-246.us-east-2.compute.internal@REALM.EXAMPLE.COM>
    # ip-172-31-37-15.us-east-2.compute.internal:
      # broker_id: 2
    # ip-172-31-34-231.us-east-2.compute.internal:
      # broker_id: 1
schema_registry:
  hosts:
    kafka3.ivan.ps.confluent.io:
      ## For kerberos sasl protocol, EACH host will need these two variables:
      # schema_registry_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/schemaregistry-ip-172-31-34-246.us-east-2.compute.internal
      # schema_registry_kerberos_principal: The principal configured in kdc server ex: schemaregistry/ip-172-31-34-246.us-east-2.compute.internal@REALM.EXAMPLE.COM>
kafka_connect:
  hosts:
    kafka2.ivan.ps.confluent.io:

      ## For kerberos sasl protocol, EACH host will need these two variables:
      # kafka_connect_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/connect-ip-172-31-34-246.us-east-2.compute.internal
      # kafka_connect_kerberos_principal: The principal configured in kdc server ex: connect/ip-172-31-34-246.us-east-2.compute.internal@REALM.EXAMPLE.COM>
#kafka_rest:
#  hosts:
#    ip-172-31-34-246.us-east-2.compute.internal:
      ## For kerberos sasl protocol, EACH host will need these two variables:
      # kafka_rest_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/restproxy-ip-172-31-34-246.us-east-2.compute.internal
      # kafka_rest_kerberos_principal: The principal configured in kdc server ex: restproxy/ip-172-31-34-246.us-east-2.compute.internal@REALM.EXAMPLE.COM>
ksql:
  hosts:
    kafka3.ivan.ps.confluent.io:
      ## For kerberos sasl protocol, EACH host will need these two variables:
      # ksql_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/ksql-ip-172-31-37-15.us-east-2.compute.internal
      # ksql_kerberos_principal: The principal configured in kdc server ex: ksql/ip-172-31-37-15.us-east-2.compute.internal@REALM.EXAMPLE.COM>
control_center:
  hosts:
    kafka1.ivan.ps.confluent.io:
      ## For kerberos sasl protocol, EACH host will need these two variables:
      # control_center_kerberos_keytab_path: <The path on ansible host to keytab file, eg. /tmp/keytabs/controlcenter-ip-172-31-37-15.us-east-2.compute.internal
      # control_center_kerberos_principal: The principal configured in kdc server ex: controlcenter/ip-172-31-37-15.us-east-2.compute.internal@REALM.EXAMPLE.COM>
